{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "Num GPUs Available: 1\n",
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "#Necessary Imports\n",
    "from pacman import GhostRules, PacmanRules, ClassicGameRules, GameState\n",
    "from game import GameStateData\n",
    "from game import Game\n",
    "from game import Directions\n",
    "from game import Actions\n",
    "from game import Agent\n",
    "from util import nearestPoint\n",
    "from util import manhattanDistance\n",
    "import util, layout\n",
    "import sys, types, time, random, os\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "from TrainingStuffs import *\n",
    "import numpy as np\n",
    "from Constants import *\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \" + str(len(tf.config.experimental.list_physical_devices('GPU'))))\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(str(len(gpus)) + \" Physical GPUs, \" + str(len(logical_gpus)) + \" Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pacman Agent, or custom defined if needed\n",
    "# @ Sergey, key thing to note, you should \n",
    "# always check if a resulting action is legal\n",
    "# Else just have pacman stay in place\n",
    "# This basically allows the network to just keep \n",
    "# Pacman in a cubby for a bit\n",
    "class SmartAgent(Agent):\n",
    "    \n",
    "    def __init__(self, create_model, temperature, train=False):\n",
    "        self.model = create_model()\n",
    "        self.is_train = train\n",
    "        self.temperature = temperature\n",
    "        if self.is_train:\n",
    "            self.target_model = create_model()\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "            self.replay_memory = deque(maxlen=REPLAY_MEMORY_SIZE)\n",
    "            self.target_update_counter = 0\n",
    "        \n",
    "        self.last_input = None\n",
    "    \n",
    "    def getAction(self, state):\n",
    "        network_input = convert_state_to_input(state, self.last_input)\n",
    "        self.last_input = network_input\n",
    "        \n",
    "        predictions = self.model.predict(np.array([network_input]))[0]\n",
    "        probs = tf.nn.softmax(predictions).numpy()\n",
    "        \n",
    "        new_probs = np.zeros(probs.shape)\n",
    "        prob_sum = 0.0\n",
    "        power = 1.0 / self.temperature\n",
    "        for i in range(len(probs)):\n",
    "            p = probs[i] ** power\n",
    "            prob_sum += p\n",
    "            new_probs[i] = p\n",
    "        probs = new_probs / prob_sum\n",
    "        \n",
    "        move = select_from_distribution(probs)\n",
    "        \n",
    "        action = [Directions.NORTH, Directions.EAST, Directions.SOUTH, Directions.WEST][move]\n",
    "        if action in state.getLegalPacmanActions():\n",
    "            return action\n",
    "        else:\n",
    "            return Directions.STOP\n",
    "        \n",
    "    def update_memory(self, action, next_state, reward, done):\n",
    "        if self.is_train:\n",
    "            self.replay_memory.append((self.last_input, action, convert_state_to_input(next_state, self.last_input), reward, done))\n",
    "    \n",
    "    def train(self, is_terminal_state):\n",
    "        if not self.is_train:\n",
    "            return\n",
    "        if len(self.replay_memory) < MIN_REPLAY_MEMORY_SIZE:\n",
    "            return\n",
    "        minibatch = random.sample(self.replay_memory, MINIBATCH_SIZE)\n",
    "        \n",
    "        current_states = np.array([transition[0] for transition in minibatch])\n",
    "        current_qs_list = self.model.predict(current_states)\n",
    "        \n",
    "        new_current_states = np.array([transition[2] for transition in minibatch])\n",
    "        future_qs_list = self.target_model.predict(new_current_states)\n",
    "\n",
    "        X = []\n",
    "        y = []\n",
    "        \n",
    "        actionIndexMap = {\n",
    "            Directions.NORTH: 0, \n",
    "            Directions.EAST: 1, \n",
    "            Directions.SOUTH: 2, \n",
    "            Directions.WEST: 3\n",
    "        }\n",
    "\n",
    "        # Now we need to enumerate our batches\n",
    "        for index, (current_state, action, new_current_state, reward, done) in enumerate(minibatch):\n",
    "\n",
    "            # If not a terminal state, get new q from future states, otherwise set it to 0\n",
    "            # almost like with Q Learning, but we use just part of equation here\n",
    "            if not done:\n",
    "                max_future_q = np.max(future_qs_list[index])\n",
    "                new_q = reward + DISCOUNT * max_future_q\n",
    "            else:\n",
    "                new_q = reward\n",
    "\n",
    "            # Update Q value for given state\n",
    "            current_qs = current_qs_list[index]\n",
    "            if action != Directions.STOP:\n",
    "                current_qs[actionIndexMap[action]] = new_q\n",
    "\n",
    "            # And append to our training data\n",
    "            X.append(current_state)\n",
    "            y.append(current_qs)\n",
    "\n",
    "        # Fit on all samples as one batch, log only on terminal state\n",
    "        self.model.fit(np.array(X), np.array(y), batch_size=MINIBATCH_SIZE, verbose=0, shuffle=False)\n",
    "\n",
    "        # Update target network counter every episode\n",
    "        if is_terminal_state:\n",
    "            self.target_update_counter += 1\n",
    "\n",
    "        # If counter reaches set value, update target network with weights of main network\n",
    "        if self.target_update_counter >= UPDATE_TARGET_EVERY:\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "            self.target_update_counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGhostAgent( Agent ):\n",
    "    def __init__( self, index ):\n",
    "        self.index = index\n",
    "\n",
    "    def getAction( self, state ):\n",
    "        action = Directions.EAST # Replace This with a call to the model\n",
    "        if action in state.getLegalActions( self.index ):\n",
    "            return action\n",
    "        else:\n",
    "            return random.choice( state.getLegalActions( self.index ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the ghost agent ___otherwise import it here. \n",
    "class SmartGhost( MyGhostAgent ):\n",
    "    def __init__( self, index ):\n",
    "        self.index = index\n",
    "\n",
    "    def getAction( self, state ):\n",
    "        action = Directions.EAST # Replace This with a call to the model\n",
    "        if action in state.getLegalActions( self.index ):\n",
    "            return action\n",
    "        else:\n",
    "            return random.choice( state.getLegalActions( self.index ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dict()\n",
    "args['layout'] = layout.getLayout(\"originalClassic.lay\")\n",
    "if args['layout'] == None: raise Exception(\"The layout \" + options.layout + \" cannot be found\")\n",
    "\n",
    "def create_model_sequential_api():\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Conv2D(filters=16,\n",
    "                            kernel_size=3,\n",
    "                            strides=(1, 1),\n",
    "                            data_format=\"channels_first\",\n",
    "                            activation=\"relu\",\n",
    "                            input_shape=(TIMESTEP_PLANES*INPUT_TIMESTEPS, PLANE_WIDTH, PLANE_HEIGHT)),\n",
    "        keras.layers.Flatten(data_format=\"channels_first\"),\n",
    "        keras.layers.Dense(128, activation='relu'),\n",
    "        keras.layers.Dense(4, activation='tanh')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.SGD(lr=1e-4, decay=0, momentum=0),\n",
    "        loss=keras.losses.categorical_crossentropy,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "    \n",
    "args['pacman'] = SmartAgent(create_model_sequential_api, 1, train=True)\n",
    "\n",
    "ghosts = [SmartGhost, MyGhostAgent,MyGhostAgent,MyGhostAgent]\n",
    "args['ghosts'] = [ghosts[i](i+1) for i in range(len(ghosts))]\n",
    "\n",
    "args['numTraining'] = 0\n",
    "args['numGames'] = 100\n",
    "args['record'] = True\n",
    "args['catchExceptions'] = False\n",
    "args['timeout'] = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a \"run\" function for the iterations\n",
    "import textDisplay\n",
    "def runGames( layout, pacman, ghosts, numGames, record, numTraining = 0, catchExceptions=False, timeout=30 ):\n",
    "    gameDisplay = textDisplay.NullGraphics()\n",
    "    rules = ClassicGameRules(timeout)\n",
    "    rules.quiet = True\n",
    "    games = []\n",
    "\n",
    "    for i in range( numGames ):\n",
    "        beQuiet = i < numTraining\n",
    "        game = rules.newGame( layout, pacman, ghosts, gameDisplay, beQuiet, catchExceptions)\n",
    "        game.run()\n",
    "        if not beQuiet: games.append(game)\n",
    "\n",
    "        if record:\n",
    "            import time, cPickle\n",
    "            fname = ('recorded_games/recorded-game-%d' % (i + 1)) +  '-'.join([str(t) for t in time.localtime()[1:6]])\n",
    "            f = file(fname, 'w')\n",
    "            components = {'layout': layout, 'actions': game.moveHistory}\n",
    "            cPickle.dump(components, f)\n",
    "            f.close()\n",
    "\n",
    "    if (numGames-numTraining) > 0:\n",
    "        scores = [game.state.getScore() for game in games]\n",
    "        wins = [game.state.isWin() for game in games]\n",
    "        winRate = wins.count(True)/ float(len(wins))\n",
    "        #HNere is where you propogate the error hum\n",
    "        # oh well\n",
    "        \n",
    "        print 'Average Score:', sum(scores) / float(len(scores))\n",
    "        print 'Scores:       ', ', '.join([str(score) for score in scores])\n",
    "        print 'Win Rate:      %d/%d (%.2f)' % (wins.count(True), len(wins), winRate)\n",
    "        print 'Record:       ', ', '.join([ ['Loss', 'Win'][int(w)] for w in wins])\n",
    "\n",
    "    return games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pacman died! Score: -504\n",
      "Pacman died! Score: -493\n",
      "Pacman died! Score: -521\n",
      "Pacman died! Score: -456\n",
      "Pacman died! Score: -479\n",
      "Pacman died! Score: -479\n",
      "Pacman died! Score: -501\n",
      "Pacman died! Score: -445\n",
      "Pacman died! Score: -484\n",
      "Pacman died! Score: -513\n",
      "Pacman died! Score: -501\n",
      "Pacman died! Score: -539\n"
     ]
    }
   ],
   "source": [
    "runGames(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Option 2: More fine grain control\n",
    "# This would require me to rewrite the run method. This is ok too, just need a heads up. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
